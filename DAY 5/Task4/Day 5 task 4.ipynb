{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tokenized Documents: [['bumrah', 'to', 'kohli', 'yorker', '145', 'kmh', 'smashed', 'for', 'a', 'four'], ['shami', 'to', 'smith', 'bouncer', '140', 'kmh', 'single', 'taken'], ['starc', 'to', 'rohit', 'full', 'toss', '147', 'kmh', 'six'], ['bumrah', 'to', 'rohit', 'yorker', '145', 'kmh', 'defended', 'solidly']]\n",
      "\n",
      " TF Values: [{'bumrah': 0.1, 'to': 0.1, 'kohli': 0.1, 'yorker': 0.1, '145': 0.1, 'kmh': 0.1, 'smashed': 0.1, 'for': 0.1, 'a': 0.1, 'four': 0.1}, {'shami': 0.125, 'to': 0.125, 'smith': 0.125, 'bouncer': 0.125, '140': 0.125, 'kmh': 0.125, 'single': 0.125, 'taken': 0.125}, {'starc': 0.125, 'to': 0.125, 'rohit': 0.125, 'full': 0.125, 'toss': 0.125, '147': 0.125, 'kmh': 0.125, 'six': 0.125}, {'bumrah': 0.125, 'to': 0.125, 'rohit': 0.125, 'yorker': 0.125, '145': 0.125, 'kmh': 0.125, 'defended': 0.125, 'solidly': 0.125}]\n",
      "\n",
      " IDF Values: {'kmh': 1.0, 'to': 1.0, 'taken': 1.916290731874155, 'smashed': 1.916290731874155, 'starc': 1.916290731874155, 'kohli': 1.916290731874155, 'bouncer': 1.916290731874155, 'toss': 1.916290731874155, 'bumrah': 1.5108256237659907, 'solidly': 1.916290731874155, 'yorker': 1.5108256237659907, 'a': 1.916290731874155, 'rohit': 1.5108256237659907, 'for': 1.916290731874155, 'full': 1.916290731874155, 'defended': 1.916290731874155, '140': 1.916290731874155, 'single': 1.916290731874155, 'four': 1.916290731874155, 'smith': 1.916290731874155, '147': 1.916290731874155, '145': 1.5108256237659907, 'shami': 1.916290731874155, 'six': 1.916290731874155}\n",
      "\n",
      " TF-IDF Scores: [{'bumrah': 0.1510825623765991, 'to': 0.1, 'kohli': 0.1916290731874155, 'yorker': 0.1510825623765991, '145': 0.1510825623765991, 'kmh': 0.1, 'smashed': 0.1916290731874155, 'for': 0.1916290731874155, 'a': 0.1916290731874155, 'four': 0.1916290731874155}, {'shami': 0.2395363414842694, 'to': 0.125, 'smith': 0.2395363414842694, 'bouncer': 0.2395363414842694, '140': 0.2395363414842694, 'kmh': 0.125, 'single': 0.2395363414842694, 'taken': 0.2395363414842694}, {'starc': 0.2395363414842694, 'to': 0.125, 'rohit': 0.18885320297074884, 'full': 0.2395363414842694, 'toss': 0.2395363414842694, '147': 0.2395363414842694, 'kmh': 0.125, 'six': 0.2395363414842694}, {'bumrah': 0.18885320297074884, 'to': 0.125, 'rohit': 0.18885320297074884, 'yorker': 0.18885320297074884, '145': 0.18885320297074884, 'kmh': 0.125, 'defended': 0.2395363414842694, 'solidly': 0.2395363414842694}]\n",
      "\n",
      " Feature Names: ['140' '145' '147' 'bouncer' 'bumrah' 'defended' 'for' 'four' 'full' 'km'\n",
      " 'kohli' 'rohit' 'shami' 'single' 'six' 'smashed' 'smith' 'solidly'\n",
      " 'starc' 'taken' 'to' 'toss' 'yorker']\n",
      "\n",
      " TF-IDF Matrix:\n",
      " [[0.         1.69314718 0.         0.         1.69314718 0.\n",
      "  2.38629436 2.38629436 0.         1.         2.38629436 0.\n",
      "  0.         0.         0.         2.38629436 0.         0.\n",
      "  0.         0.         1.         0.         1.69314718]\n",
      " [2.38629436 0.         0.         2.38629436 0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  2.38629436 2.38629436 0.         0.         2.38629436 0.\n",
      "  0.         2.38629436 1.         0.         0.        ]\n",
      " [0.         0.         2.38629436 0.         0.         0.\n",
      "  0.         0.         2.38629436 1.         0.         1.69314718\n",
      "  0.         0.         2.38629436 0.         0.         0.\n",
      "  2.38629436 0.         1.         2.38629436 0.        ]\n",
      " [0.         1.69314718 0.         0.         1.69314718 2.38629436\n",
      "  0.         0.         0.         1.         0.         1.69314718\n",
      "  0.         0.         0.         0.         0.         2.38629436\n",
      "  0.         0.         1.         0.         1.69314718]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample Commentary Dataset\n",
    "commentary = [\n",
    "    \"Bumrah to Kohli, Yorker, 145 km/h, smashed for a Four!\",\n",
    "    \"Shami to Smith, Bouncer, 140 km/h, single taken.\",\n",
    "    \"Starc to Rohit, Full Toss, 147 km/h, Six!\",\n",
    "    \"Bumrah to Rohit, Yorker, 145 km/h, defended solidly.\",\n",
    "]\n",
    "\n",
    "# ------------------ 1️ Preprocess Text ------------------\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove punctuation\n",
    "    return text.split()  # Tokenize\n",
    "\n",
    "tokenized_docs = [preprocess(doc) for doc in commentary]\n",
    "\n",
    "# ------------------ 2️ Compute Term Frequency (TF) ------------------\n",
    "def compute_tf(doc):\n",
    "    tf_dict = Counter(doc)\n",
    "    total_words = len(doc)\n",
    "    return {word: freq / total_words for word, freq in tf_dict.items()}\n",
    "\n",
    "tf_documents = [compute_tf(doc) for doc in tokenized_docs]\n",
    "\n",
    "# ------------------ 3️ Compute Inverse Document Frequency (IDF) ------------------\n",
    "def compute_idf(doc_list):\n",
    "    N = len(doc_list)  # Total number of documents\n",
    "    idf_dict = {}\n",
    "    all_words = set(word for doc in doc_list for word in doc)  # Unique words\n",
    "\n",
    "    for word in all_words:\n",
    "        doc_count = sum(1 for doc in doc_list if word in doc)\n",
    "        idf_dict[word] = math.log((N + 1) / (doc_count + 1)) + 1  # Smoothed IDF\n",
    "\n",
    "    return idf_dict\n",
    "\n",
    "idf_values = compute_idf(tokenized_docs)\n",
    "\n",
    "# ------------------ 4️ Compute TF-IDF ------------------\n",
    "def compute_tfidf(tf_docs, idf_values):\n",
    "    tfidf_documents = []\n",
    "    for tf_doc in tf_docs:\n",
    "        tfidf_doc = {word: tf_value * idf_values[word] for word, tf_value in tf_doc.items()}\n",
    "        tfidf_documents.append(tfidf_doc)\n",
    "    return tfidf_documents\n",
    "\n",
    "tfidf_documents = compute_tfidf(tf_documents, idf_values)\n",
    "\n",
    "# ------------------ 5️ Verify Using Scikit-learn ------------------\n",
    "vectorizer = TfidfVectorizer(smooth_idf=False, norm=None)  # Disables smoothing & normalization\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(commentary)\n",
    "\n",
    "# ------------------  Display Outputs ------------------\n",
    "print(\"\\n Tokenized Documents:\", tokenized_docs)\n",
    "\n",
    "print(\"\\n TF Values:\", tf_documents)\n",
    "\n",
    "print(\"\\n IDF Values:\", idf_values)\n",
    "\n",
    "print(\"\\n TF-IDF Scores:\", tfidf_documents)\n",
    "\n",
    "print(\"\\n Feature Names:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\n TF-IDF Matrix:\\n\", X_tfidf.toarray())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
